import osimport mathimport numpy as npimport argparseimport torchvision.transforms as transformsfrom torchvision.utils import save_imagefrom torch.utils.data import DataLoader ,Datasetfrom torchvision import datasetsfrom torch.autograd import Variableimport torch.nn as nnimport torch.nn.functional as Fimport torchimport matplotlib.pyplot as pltfrom torch.utils import datafrom PIL import Imagedevice = torch.device('cuda:0')opt = {}opt['n_epochs'] = 200  # 迭代次数opt['batch_size'] = 64#64opt['lr'] = 0.0002  # adam: 学习率opt['b1'] = 0.5  # adam: 梯度的一阶动量衰减 momentumopt['b2'] = 0.999  # adam: 梯度的一阶动量衰减 momentumopt['latent_dim'] = 100 # latent空间的维数opt['img_size'] = 28 # 28opt['channels'] = 1opt['sample_interval'] = 400  # 图像采样间隔(做记录)# 输入图片大小img_shape = (opt['channels'], opt['img_size'], opt['img_size'])cuda = True if torch.cuda.is_available() else Falseprint(cuda)Tensor = torch.cuda.FloatTensoros.makedirs("images", exist_ok=True)os.makedirs("images/1", exist_ok=True)os.makedirs("mnist", exist_ok=True)# 下载图片transforms = transforms.Compose([transforms.Resize(opt['img_size']), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])dataset = datasets.MNIST("mnist",train=True,download=True,transform=transforms)dataloader = torch.utils.data.DataLoader(dataset,batch_size=opt['batch_size'],shuffle=1,)# 生成器模型class Generator(nn.Module):    def __init__(self):        super(Generator, self).__init__()        # 整个作为一层        def block(in_feat, out_feat, normalize=True):            layers = [nn.Linear(in_feat, out_feat)]            if normalize:                layers.append(nn.BatchNorm1d(out_feat, 0.8))            layers.append(nn.LeakyReLU(0.2, inplace=True))  # inplace一个原地操作            # 是对于Conv2d这样的上层网络传递下来的tensor直接进行修改，好处就是可以节省运算内存，不用多储存变量y            return layers        # 这里前面加*相当于在Sequential中extend        self.model = nn.Sequential(            *block(opt['latent_dim'], 128, normalize=False),            *block(128, 256),            *block(256, 512),            *block(512, 1024),            nn.Linear(1024, int(np.prod(img_shape))),  # np.prod摊开            nn.Tanh()        )    def forward(self, data):        img = self.model(data)        img = img.view(img.size(0), *img_shape)  # 将平铺的数据变为图片(图片数, 长， 宽，channel)        return img# 定义判别函数class Discriminator(nn.Module):    def __init__(self):        super(Discriminator, self).__init__()        self.model = nn.Sequential(nn.Linear(int(np.prod(img_shape)), 512),                                   nn.LeakyReLU(0.2, inplace=True),                                   nn.Linear(512, 256),                                   nn.LeakyReLU(0.2, inplace=True),                                   nn.Linear(256, 1),                                   nn.Sigmoid(),                                   )    def forward(self, img):        img_flat = img.view(img.size(0), -1)        # 拉成 图片1 维度相乘        #     图片n 维度相乘        validity = self.model(img_flat)        return validity# 判别器损失函数adversarial_loss = torch.nn.BCELoss()# 初始化生成器和判别器generator = Generator().to(device)discriminator = Discriminator().to(device)# 优化器optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt['lr'], betas=(opt['b1'], opt['b2']))optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt['lr'], betas=(opt['b1'], opt['b2']))# 训练模型g_cost = []d_cost = []for epoch in range(opt['n_epochs']):    for i, (imgs, _) in enumerate(dataloader):        np.random.seed(0)        valid = Tensor(imgs.size(0), 1).fill_(1.0)  # 真实图片类标签        valid.requires_grad = False        fake = Tensor(imgs.size(0), 1).fill_(0.0)  # 假图片类标签        fake.requires_grad = False        # 将数据转成cuda的tensor        real_imgs = imgs.type(Tensor).to(device)        # 训练生成器        optimizer_G.zero_grad()  # 生成器梯度清零        # 产生噪声数据 64张100维噪声        noise = Tensor(np.random.normal(0, 1, (imgs.shape[0], opt['latent_dim'])))        # 生成噪声图片        fake_img = generator(noise)        # 更新生层器        g_loss = adversarial_loss(discriminator(fake_img), valid)        g_loss.backward()        optimizer_G.step()        # 训练判别器        optimizer_D.zero_grad()        real_loss = adversarial_loss(discriminator(real_imgs), valid)        # detach返回一个新的张量，它与当前图形分离。结果永远不需要梯度        fake_loss = adversarial_loss(discriminator(fake_img.detach()), fake)        d_loss = (real_loss + fake_loss) / 2        d_loss.backward()        optimizer_D.step()        if i % 500 == 0:            g_cost.append(g_loss)            d_cost.append(d_loss)            print(                "[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]"                % (epoch, opt['n_epochs'], i, len(dataloader), d_loss.item(), g_loss.item())            )        batches_done = epoch * len(dataloader) + i        if batches_done % opt['sample_interval'] == 0:            save_image(fake_img.data[:25], "images/%d.png" % batches_done, nrow=5, normalize=True)plt.plot(np.squeeze(g_cost))plt.plot(np.squeeze(d_cost))plt.ylabel('cost')plt.xlabel('iterations')plt.title("g/d_cost")plt.show()